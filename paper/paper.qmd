---
title: "The Effects of Homeless Shelter Occupancy on Non-Fatal Opioid Overdoses"
subtitle: "My subtitle if needed"
author: 
  - Justin Klip
thanks: "Code and data are available at: [https://github.com/justinklip/toronto_shelter_overdose_study)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
library(arrow)
library(tidyverse)
library(here)
```

# Introduction

Overview Paragraph: Opioid overdoses have been a prominent issue in North America for some time. Opioid-related deaths have doubled in Canada from 2019 to the end of 2021 [@Bains_2024] .. Homeless opioid overdoses in shelters have climbed as well during the pandemic, with opioid deaths in Ontario shelters more than tripling during the pandemic [@Alhmidi_2024]. While many avenues could be explored for this climb, one area to potentially look at is the effects of the shelter conditions themselves.

The remainder of this paper is structured as follows. @sec-data describes how the data was acquired, how it was measured, and provides visualizations on the overdoses and occupancy in shelters.

# Data {#sec-data}

## Data Source

I make use of two data sources in order to run my analysis. Firstly I make use of Open Data Toronto's Daily Shelter Overnight Capacity Data . Secondly I use Open Data Toronto's Fatal and Non-Fatal Suspected Opioid Overdoses in the Shelter System data set (cite). These data sets compile

Using the statistical programming language R [@citeR], the data was then cleaned, tested, and merged using (cite packages). The daily shelter attendance data was aggregated from the daily data in the shelter data base to a quarterly record in order to merge with the overdose data set. Details are in the appendix on how exactly observations were dropped and merged (cross-reference appendix). Some dummy variables were added into the data set in order to account for selection induced upon dropping missing values. Data such as

Overview text

## Measurement

The Daily Shelter Overnight Capacity Data from Open Data Toronto is measured directly from administrative data (cite). According to Open Data Toronto, this administrative data comes from the Toronto Shelter and Support Services division's Shelter Management Information (SMIS) database. The data is generated as follows: a shelter records metrics in their shelter at exactly 4:00am every day (likely through a computer or check-in system) such as attendance numbers, capacity numbers, and respective capacity utilization rates, then uploads that data daily in compliance with the SMIS data requirements. Generally shelters measured occupancy and capacity in one of two ways: beds or rooms. Bed-based occupancy was used in shelters that had communal sleeping areas, whereas room-based occupancy was used for more family-oriented programs. An "individual" in this data set would counts as someone who was served in a service area or program (https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/shelter-census/). Interestingly data is only recorded for the first quarter of 2023, this and other limitations of the data are discussed further in the (@appendix)

The Fatal and Non-Fatal Suspected Opioid Overdoses in the Shelter System Data Set (cite) comes from paramedic rather than shelter level data. This data only counts for specific kinds of shelters: shelter-hotels, emergency shelters, and shelter-hotels, meaning that when the data is merged, only shelters coming from this data set will be used. An entry in the data set is formed as follows: a shelter member or employee calls 911 for an emergency, when Paramedics arrive and if they determine on the scene that this is a suspected overdose, then that location gets an observation added to the data set. It's important to note also that if a particular address has less than 5 non-fatal overdoses in a particular quarter, then the true amount is not published for anonymity purposes. This data set also includes fatal overdoses, and a fatal overdose is only added as an entry if the Coroner's office determines the cause of death to be an overdose. This data is also not matched at the shelter level and is only aggregate statistics, so it will be ignored for the purpose of analysis since I am more interested in the relationship between capacity and overdoses.

## Summary Statistics

```{r}
#| fig-label: tbl-1
#| echo: false
#| warning: false

# Set the path to your parquet file
file_path <- here("data", "02-analysis_data", "shelter_analysis_data.parquet")

# Read the Parquet file into a dataframe
shelter_analysis_data <- read_parquet(file_path)

library(dplyr)
library(knitr)
library(tidyr)

# First, calculate the summary statistics, applying the <5 filter only to suspected_non_fatal_overdoses
summary_stats <- shelter_analysis_data %>%
  filter(suspected_non_fatal_overdoses != "< 5" & !is.na(suspected_non_fatal_overdoses)) %>%
  summarise(
    obs_suspected_overdoses = sum(!is.na(suspected_non_fatal_overdoses) & suspected_non_fatal_overdoses != "< 5"),
    mean_suspected_overdoses = mean(as.numeric(suspected_non_fatal_overdoses), na.rm = TRUE),
    median_suspected_overdoses = median(as.numeric(suspected_non_fatal_overdoses), na.rm = TRUE),
    sd_suspected_overdoses = sd(as.numeric(suspected_non_fatal_overdoses), na.rm = TRUE),
    min_suspected_overdoses = min(as.numeric(suspected_non_fatal_overdoses), na.rm = TRUE),
    max_suspected_overdoses = max(as.numeric(suspected_non_fatal_overdoses), na.rm = TRUE),
    
    obs_avg_occupancy_rate = sum(!is.na(avg_occupancy_rate)),
    mean_avg_occupancy_rate = mean(avg_occupancy_rate, na.rm = TRUE),
    median_avg_occupancy_rate = median(avg_occupancy_rate, na.rm = TRUE),
    sd_avg_occupancy_rate = sd(avg_occupancy_rate, na.rm = TRUE),
    min_avg_occupancy_rate = min(avg_occupancy_rate, na.rm = TRUE),
    max_avg_occupancy_rate = max(avg_occupancy_rate, na.rm = TRUE),
    
    obs_daily_avg_users = sum(!is.na(daily_avg_users)),
    mean_daily_avg_users = mean(daily_avg_users, na.rm = TRUE),
    median_daily_avg_users = median(daily_avg_users, na.rm = TRUE),
    sd_daily_avg_users = sd(daily_avg_users, na.rm = TRUE),
    min_daily_avg_users = min(daily_avg_users, na.rm = TRUE),
    max_daily_avg_users = max(daily_avg_users, na.rm = TRUE)
  )

# Now, create a tidy summary table where each variable gets its own row
summary_stats_table <- bind_rows(
  tibble(Variable = "Non-Fatal Overdoses", 
         Observations = summary_stats$obs_suspected_overdoses, 
         Mean = summary_stats$mean_suspected_overdoses, 
         Median = summary_stats$median_suspected_overdoses, 
         `Standard Deviation` = summary_stats$sd_suspected_overdoses, 
         Minimum = summary_stats$min_suspected_overdoses, 
         Maximum = summary_stats$max_suspected_overdoses),
  
  tibble(Variable = "Average Occupancy Rate", 
         Observations = sum(!is.na(shelter_analysis_data$avg_occupancy_rate)), 
         Mean = mean(shelter_analysis_data$avg_occupancy_rate, na.rm = TRUE), 
         Median = median(shelter_analysis_data$avg_occupancy_rate, na.rm = TRUE), 
         `Standard Deviation` = sd(shelter_analysis_data$avg_occupancy_rate, na.rm = TRUE), 
         Minimum = min(shelter_analysis_data$avg_occupancy_rate, na.rm = TRUE), 
         Maximum = max(shelter_analysis_data$avg_occupancy_rate, na.rm = TRUE)),
  
  tibble(Variable = "Daily Average Users", 
         Observations = sum(!is.na(shelter_analysis_data$daily_avg_users)), 
         Mean = mean(shelter_analysis_data$daily_avg_users, na.rm = TRUE), 
         Median = median(shelter_analysis_data$daily_avg_users, na.rm = TRUE), 
         `Standard Deviation` = sd(shelter_analysis_data$daily_avg_users, na.rm = TRUE), 
         Minimum = min(shelter_analysis_data$daily_avg_users, na.rm = TRUE), 
         Maximum = max(shelter_analysis_data$daily_avg_users, na.rm = TRUE))
)

# Print the summary statistics as a table using knitr
kable(summary_stats_table, caption = "Summary Statistics for Non-Fatal Overdoses, Average Occupancy Rate, and Daily Average Users", format = "markdown")
```

@tbl-1 Gives the summary statistics of our variables of interest. We can see that most of the occupancy rate is extremely high and centered near 96 percent for the mean and 99 percent for the median, although there is some standard deviation. As for daily average users, the median amount daily is about 44, suggesting most shelters are about medium size. The really low minimum amount here is likely seen by a shelter that was temporarily open, as averages are calculated by dividing by the number of days in the quarter. The mean is much higher than the median at 71 meaning that some shelters are a lot larger, dragging the average up. This is indicated by the maximum daily average users of one shelter being and the large standard deviation. The non-fatal overdoses (excluding entries labelled "\<5") are centered around 5, suggesting the true median is likely lower and within that less than 5 category.

## Outcome variables

```{r}
#| fig-label: fig-1
#| fig-cap: Shelters Overdoses by Quarter-Year By Data Category
#| echo: false
#| warning: false

library(arrow)
library(tidyverse)
library(here)
library(ggplot2)

# Set the path to your parquet file
file_path <- here("data", "02-analysis_data", "shelter_analysis_data.parquet")

# Read the Parquet file into a dataframe
shelter_analysis_data <- read_parquet(file_path)

# Create a new column for overdose categories ("0", "<5", ">5")
shelter_analysis_data <- shelter_analysis_data %>%
  mutate(overdose_category = case_when(
    suspected_non_fatal_overdoses == "< 5" ~ "<5",
    suspected_non_fatal_overdoses == "0" ~ "0",
    suspected_non_fatal_overdoses != "< 5" & suspected_non_fatal_overdoses != "0" & suspected_non_fatal_overdoses != "" ~ ">5",
    TRUE ~ NA_character_
  ))

# Create a new column for numeric overdoses (replace "<5" with NA for now)
shelter_analysis_data$non_fatal_overdoses_numeric <- 
  ifelse(shelter_analysis_data$suspected_non_fatal_overdoses == "< 5", NA, 
         as.numeric(shelter_analysis_data$suspected_non_fatal_overdoses))

# Remove duplicates: Keep only one row per shelter per quarter-year
shelter_analysis_data_unique <- shelter_analysis_data %>%
  distinct(year, year_stage, address, .keep_all = TRUE)  # Assuming 'shelter_id' identifies shelters

# Aggregate data by year, quarter, and overdose category
overdose_summary <- shelter_analysis_data_unique %>%
  group_by(year, year_stage, overdose_category) %>%
  summarise(count = n(), .groups = 'drop')  # Count number of shelters per category

# Convert year and quarter to a single "quarter-year" column for easier plotting
overdose_summary <- overdose_summary %>%
  mutate(quarter_year = paste(year, "", year_stage, sep = "-"))

# Reorder overdose_category so "0" is at the bottom, "<5" next, and ">5" on top
shelter_analysis_data <- shelter_analysis_data %>%
  mutate(overdose_category = factor(overdose_category, levels = c("0", "<5", ">5")))

# Re-aggregate data by year, quarter, and overdose category
overdose_summary <- shelter_analysis_data_unique %>%
  group_by(year, year_stage, overdose_category) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(quarter_year = paste(year, year_stage, sep = "-"))

# Plot the bar chart with the reordered stack
ggplot(overdose_summary, aes(x = quarter_year, y = count, fill = overdose_category)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(
    aes(label = ifelse(count > 0, count, "")),  # Only show labels for non-zero counts
    position = position_stack(vjust = 0.5),    # Center labels within each segment
    size = 3                                   # Adjust text size as needed
  ) +
  labs(
    x = "Quarter-Year",
    y = "Number of Shelters",
    fill = "Overdose Category"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("0" = "green", "<5" = "lightblue", ">5" = "red")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```

@fig-1 Provides information on the number of overdoses for each shelter in the data set from quarter to quarter. It is important to note that the data set does not include Q4 data. The first thing to note is that the number of shelters seems to decrease in 2023, this is likely consistent with COVID-19 specific shelters beginning to close. Another important thing to note is that the number of overdoses greater than 5 and the amount less than 5 are generally split evenly between the shelters. This suggests we have some "high overdose" level shelters, likely because they are bigger, and others which are smaller and less drug prone.

```{r}
#| fig-label: fig-2
#| fig-cap: Distribution of Overdoses Counts, Shelter-By-Quarter, excluding nonzero less than 5 counts.
#| echo: false
#| warning: false

# Set the path to your parquet file
file_path <- here("data", "02-analysis_data", "shelter_analysis_data.parquet")

# Read the Parquet file into a dataframe
shelter_analysis_data <- read_parquet(file_path)

# Filter for numeric overdoses (>5 observations) and exclude "<5" and "0"
filtered_data <- shelter_analysis_data %>%
  filter(
    suspected_non_fatal_overdoses != "< 5",                # Exclude "<5" overdoses                
    !is.na(suspected_non_fatal_overdoses)                 # Exclude missing values
  ) %>%
  mutate(
    non_fatal_overdoses_numeric = as.numeric(suspected_non_fatal_overdoses) # Convert to numeric
  )

# Create a new column for quarter-year
filtered_data <- filtered_data %>%
  mutate(quarter_year = paste(year, "Q", year_stage, sep = "-"))

# Plot the histogram for all non-0 and non-<5 observations
ggplot(filtered_data, aes(x = non_fatal_overdoses_numeric)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "black", alpha = 0.8) +
  labs(
    x = "Number of Overdoses",
    y = "Number of Shelter Quarters",
  ) +
  theme_minimal()

```

@fig-2 Plots the distribution of the number of overdoses for each quarter in each shelter, except those that are marked "\<5". The data seems to indicate a clear mass around 0, with about 150 out of approximately 500 total observations being located at this point. Considering the shape of the data, it would be reasonable to assume that there are less and less observations as the number of overdoses per quarter increases at a certain address. This motivates our decision making in section @sec-model. \## Predictor variables

```{r}
#| fig-label: fig-3
#| fig-cap: Distribution of Average Quarterly Occupancy Rates in Toronto Shelters by Type of Shelter
#| echo: false
#| warning: false
library(ggplot2)
library(dplyr)


# Filter out missing values for avg_occupancy_rate if necessary
shelter_analysis_data <- shelter_analysis_data %>%
  filter(!is.na(avg_occupancy_rate))

# Create the faceted plot
ggplot(shelter_analysis_data, aes(x = avg_occupancy_rate, fill = factor(used_avg_occupancy_rate_beds))) +
  geom_histogram(binwidth = 5, alpha = 0.7, color = "black") +
  facet_wrap(~used_avg_occupancy_rate_beds, 
             labeller = as_labeller(c("0" = "Used Rooms Level Occupancy", "1" = "Used Bed Level Occupancy"))) +
  labs(
    title = "Distribution of Average Occupancy Rate by Type of Shelter",
    x = "Average Occupancy Rate",
    y = "Count",
    fill = "Used Beds Data"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("0" = "skyblue", "1" = "orange")) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none"  # Hide legend since facet labels provide the same information
  )
```

@fig-3 Plots the distribution of the average occupancy rate by quarter-year, as seen in some quarters, some shelters do have less than 100 percent occupancy, although a lot spend their time at the 95-100 percent occupancy rate. For the bed data there are a lot more observations with less than 95 to 100 percent occupancy. This makes sense as it is a lot easier for a shelter to fill up all their rooms than it is to fill all their beds.

```{r}
#| fig-label: fig-4
#| fig-cap: Distribution of Daily Average Number of People of Toronto Homeless Shelters
#| echo: false
#| warning: false

# Create the histogram for daily_avg_users
ggplot(shelter_analysis_data, aes(x = daily_avg_users)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of Daily Average Users",
    x = "Daily Average Shelter Attendees",
    y = "Shelter Quarters"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

@fig-4 Plots the distribution of the daily average number of people in Toronto Shelters each quarter, we see that most shelters are relatively small-medium scale, with less than 100 average people staying a shelter per night on average each quarter. There are some larger shelters too, explaining why the tail of the distribution moves right, with a bunch of shelters having more than 200 daily average users each quarter.

# Model {sec-model}

Following (https://clausportner.com/blog/archives/2018/6/8/double-censored-regression-in-r) I construct a model a double-censored regression model making use of (cite survival) function in R.

Here we briefly describe the Double-Censored Regression used to investigate the impact of ... Background details and diagnostics are included in [Appendix -@sec-model-details].

### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.

# Results

Our results are summarized in @tbl-modelresults.

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {.unnumbered}

# Additional data details

# Data Limitations and Improvements

Due to data issues, my modelling left a lot to be desired. In this segment I document segments of the observational data that I would include to improve data to allow for better modelling. I also lay out a potential way causal inference could be done given the data if we were to strengthen our data in a few ways.

## Observational Data

Our approach to deal with censored values (where a shelter simply said their overdose count was less than 5 for a quarter) was simply to set the value to 1, as that would give me a lower bound estimate for the effect of shelter occupancy on overdoses. The idealized data would simply not have these censored observations, allowing the models to more accurately represent the data at hand. While privacy concerns were cited as the main reason for this, ideally I would sign some data confidentiality agreement in order to be able to use this data.

The next issue comes from the lack of information in the shelter overdose data set,

# Idealized Survey

Another way that the data could be improved would be including surveys of shelter members. This would more adequately describe living conditions in a way that the current data could not. While I try to use average occupancy rates and total attendance amounts as a proxy for 'crowdedness', arguably a better way to do this would be to ask members of these shelters themselves. This is because most shelters try to stay near 100 percent occupancy at all times, so there is not much variation for analysis using this as a predictor. Surveys could be randomly assigned to a certain percentage of members of each shelter (e.g 10%) quarterly asking a question like "what degree of privacy do you feel like you have" with a scale. I would also assure members of their anonymity to avoid biases relating to going for middle answer. Combining both the survey, as well as the the average occupancy rate as a control, with the fixed overdose data would provide much more informative results.

# Potentiality for Causal Inference Using Observational Data

# Model details {#sec-model-details}

## Posterior predictive check

## Diagnostics

\newpage

# References
